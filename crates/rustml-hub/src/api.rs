//! HuggingFace Hub API client

use crate::{HubError, HubResult};
use std::path::PathBuf;

/// HuggingFace Hub API client
#[derive(Debug, Clone)]
pub struct HubApi {
    /// Base URL for the Hub
    base_url: String,
    /// Cache directory for downloaded models
    cache_dir: PathBuf,
    /// API token (optional, for private models)
    token: Option<String>,
}

impl Default for HubApi {
    fn default() -> Self {
        Self::new()
    }
}

impl HubApi {
    /// Create a new Hub API client
    pub fn new() -> Self {
        let cache_dir = dirs::cache_dir()
            .unwrap_or_else(|| PathBuf::from("."))
            .join("rustml")
            .join("hub");

        Self {
            base_url: "https://huggingface.co".to_string(),
            cache_dir,
            token: None,
        }
    }

    /// Create with custom cache directory
    pub fn with_cache_dir(cache_dir: impl Into<PathBuf>) -> Self {
        Self {
            cache_dir: cache_dir.into(),
            ..Self::new()
        }
    }

    /// Set API token for private models
    pub fn with_token(mut self, token: impl Into<String>) -> Self {
        self.token = Some(token.into());
        self
    }

    /// Get the cache directory
    pub fn cache_dir(&self) -> &PathBuf {
        &self.cache_dir
    }

    /// Download a model from HuggingFace Hub
    ///
    /// # Arguments
    /// * `model_id` - Model identifier (e.g., "openai-community/gpt2")
    ///
    /// # Returns
    /// A `ModelBundle` containing paths to downloaded files
    pub async fn download_model(&self, model_id: &str) -> HubResult<ModelBundle> {
        let model_dir = self.cache_dir.join(model_id.replace('/', "--"));

        // Create cache directory if it doesn't exist
        tokio::fs::create_dir_all(&model_dir).await?;

        // Files to download for GPT-2
        let files = vec![
            "config.json",
            "model.safetensors",
            "vocab.json",
            "merges.txt",
            "tokenizer.json",
        ];

        for file in &files {
            let file_path = model_dir.join(file);
            if !file_path.exists() {
                self.download_file(model_id, file, &file_path).await?;
            }
        }

        Ok(ModelBundle {
            model_id: model_id.to_string(),
            model_dir,
        })
    }

    /// Download a specific file from a model repository
    async fn download_file(
        &self,
        model_id: &str,
        filename: &str,
        dest: &PathBuf,
    ) -> HubResult<()> {
        let url = format!(
            "{}/{}/resolve/main/{}",
            self.base_url, model_id, filename
        );

        let client = reqwest::Client::new();
        let mut request = client.get(&url);

        if let Some(ref token) = self.token {
            request = request.header("Authorization", format!("Bearer {}", token));
        }

        let response = request.send().await.map_err(|e| {
            HubError::NetworkError(format!("Failed to download {}: {}", filename, e))
        })?;

        if !response.status().is_success() {
            // Skip optional files
            if filename == "model.safetensors" {
                // Try pytorch_model.bin as fallback
                return Ok(());
            }
            if filename == "tokenizer.json" {
                // Tokenizer.json is optional
                return Ok(());
            }
            return Err(HubError::NetworkError(format!(
                "Failed to download {}: HTTP {}",
                filename,
                response.status()
            )));
        }

        let bytes = response.bytes().await.map_err(|e| {
            HubError::NetworkError(format!("Failed to read response: {}", e))
        })?;

        tokio::fs::write(dest, &bytes).await?;

        Ok(())
    }

    /// Check if a model is cached locally
    pub fn is_cached(&self, model_id: &str) -> bool {
        let model_dir = self.cache_dir.join(model_id.replace('/', "--"));
        model_dir.exists() && model_dir.join("config.json").exists()
    }

    /// Get a cached model bundle without downloading
    pub fn get_cached(&self, model_id: &str) -> Option<ModelBundle> {
        if self.is_cached(model_id) {
            Some(ModelBundle {
                model_id: model_id.to_string(),
                model_dir: self.cache_dir.join(model_id.replace('/', "--")),
            })
        } else {
            None
        }
    }
}

/// A bundle of downloaded model files
#[derive(Debug, Clone)]
pub struct ModelBundle {
    /// Model identifier
    pub model_id: String,
    /// Path to the model directory
    pub model_dir: PathBuf,
}

impl ModelBundle {
    /// Get path to config.json
    pub fn config_path(&self) -> PathBuf {
        self.model_dir.join("config.json")
    }

    /// Get path to model weights (SafeTensors format)
    pub fn weights_path(&self) -> PathBuf {
        self.model_dir.join("model.safetensors")
    }

    /// Get path to vocab.json
    pub fn vocab_path(&self) -> PathBuf {
        self.model_dir.join("vocab.json")
    }

    /// Get path to merges.txt
    pub fn merges_path(&self) -> PathBuf {
        self.model_dir.join("merges.txt")
    }

    /// Load model configuration
    pub async fn load_config(&self) -> HubResult<serde_json::Value> {
        let content = tokio::fs::read_to_string(self.config_path()).await?;
        serde_json::from_str(&content).map_err(|e| HubError::ParseError(e.to_string()))
    }

    /// Load tensors from the model
    pub fn load_tensors(&self) -> HubResult<std::collections::HashMap<String, rustml_core::Tensor>> {
        let loader = crate::SafeTensorLoader::new();
        loader.load(&self.weights_path())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_hub_api_creation() {
        let api = HubApi::new();
        assert!(!api.cache_dir().as_os_str().is_empty());
    }

    #[test]
    fn test_model_bundle_paths() {
        let bundle = ModelBundle {
            model_id: "test/model".to_string(),
            model_dir: PathBuf::from("/tmp/test"),
        };
        assert!(bundle.config_path().ends_with("config.json"));
        assert!(bundle.weights_path().ends_with("model.safetensors"));
    }
}
